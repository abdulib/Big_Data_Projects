{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Spark Programming on Taxicab Report Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to write some `pyspark` code that does some computation over a large dataset. Specifically, your Spark program will analyze a dataset consisting of New York City Taxi trip reports in the Year 2013. The dataset was released under the FOIL (The Freedom of Information Law) and made public by Chris Whong (https://chriswhong.com/open-data/foiling-nycs-boro-taxi-trip-data/).\n",
    "\n",
    "The dataset is a simple `csv` file. Each taxi trip report is a different line in the file. Among\n",
    "other things, each trip report includes the starting point, the drop-off point, corresponding timestamps, and\n",
    "information related to the payment. The data are reported by the time that the trip ended, i.e., upon arrive in\n",
    "the order of the drop-off timestamps.\n",
    "The attributes present on each line of the file are, in order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| attribute    | description                                                       |\n",
    "| -------------|-------------------------------------------------------------------|\n",
    "| medallion    | an md5sum of the identifier of the taxi - vehicle bound (Taxi ID) |\n",
    "| hack_license | an md5sum of the identifier for the taxi license (driver ID)      |\n",
    "| vendor_id    |identifies the vendor  |\n",
    "| pickup_datetime\t|time when the passenger(s) were picked up  |\n",
    "| payment_type\t |the payment method -credit card or cash  |\n",
    "| fare_amount\t |fare amount in dollars  |\n",
    "| surcharge\t |surcharge in dollars  |\n",
    "| mta_tax\t |tax in dollars  |\n",
    "| tip_amount\t |tip in dollars  |\n",
    "| tolls_amount\t |bridge and tunnel tolls in dollars  |\n",
    "| total_amount\t |total paid amount in dollars  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data files:\n",
    "* `taxi_small_subset.csv` - This is a subset of the entire big file. You can examine this file to see what the data look like. Also, you can use this file for running your code in a single-node platform (e.g., in Vocareum) and debug it, before running your code on the big file in the cluster.   \n",
    "* `2013_weekdays.csv` - This is a file with the dates of 365 days of the year 2013 with their corresponding week day. This file is used in task 4 to do join.\n",
    "* S3 URI `s3://comp643bucket/homework/spark_taxicab/trip*` - This is the address of the entire dataset available in S3, which is a big file (18.4 GB). Once you debugged your code on the small subset, your final task is to run your code on this big file over an EMR cluster in AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this homework, you need to complete 5 tasks described below.** \n",
    "\n",
    "**For tasks 1 through 4, write your Spark code in this Jupyter Notebook and run your code on the small subset of data, i.e., `taxi_small_subset.csv`, in Vocareum. This helps you debug your Spark program easier since you're running it in an interactive single-node platform and on a small dataset.**     \n",
    "\n",
    "**Once you've debugged your code on a small dataset, for task 5, you need to execute your Spark code for tasks 1 through 4, in an AWS EMR cluster on the big dataset that is stored in S3 (`s3://comp643bucket/homework/spark_taxicab/trip*`).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyspark works best with java8 \n",
    "# set JAVA_HOME enviroment variable to java8 path \n",
    "%env JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read the data file into an RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi = sc.textFile('data/taxi_small_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "taxi.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - clean the dataset (20 pts)\n",
    "\n",
    "Write a Spark program that reads the dataset into an RDD, splits each line by `,` to extract field values, and cleans the RDD through the following steps:\n",
    "* Remove lines with any missing value indicated by `NULL` \n",
    "* Validate the type of the following fields and remove lines with any invalid field value:\n",
    "    * `pickup_datetime` must match this pattern 'YYYY-MM-DD HH:MM:SS'\n",
    "    * All fileds in dollars (`fare_amount`, `surcharge`, `mta_tax`, `tip_amount`, `tolls_amount`, `total_amount`) must be non-negative numbers (with or without a decimal point)\n",
    "    \n",
    "After each step of cleaning, run `count()` on your RDD, to see how many lines have been left. \n",
    "\n",
    "Below, we give you a set of cells you can use to walk through the analysis procress. You are also welcome to simply write all of your code in one cell, following your own logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split each line by `,` to extract field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove lines with any `NULL` value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run `count()` on your RDD to see how many lines have been left**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove lines with `pickup_datetime` that does not match this pattern 'YYYY-MM-DD HH:MM:SS'**\n",
    "\n",
    "For this task, you can use Python `re` module along with your Spark code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run `count()` on your RDD to see how many lines have been left**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the fields indicating an amount in dollar (`fare_amount`, `surcharge`, `mta_tax`, `tip_amount`, `tolls_amount`, `total_amount`) must be non-negative numeric (with or without decimal point) value. Remove lines with any value that does not match this pattern.** \n",
    "\n",
    "For this task, you can use Python `re` module along with your Spark code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run `count()` on your RDD to see how many lines have been left**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - compute total revenue by dates (20 pts)\n",
    "\n",
    "Write a Spark program on your derived cleaned RDD (from task 1) that computes the total amount of revenue (`total_amount` field) for each date (`pickup_datetime` field without time portions - only dates). Then, sort your RDD by the total revenue in ascending order and print out the 5 lines with the smallest total revenue. That shows the 5 dates with least total revenue.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - compute total revenue by taxi drivers  (20 pts)\n",
    "\n",
    "Write a Spark program on your derived cleaned RDD (from task 1) that computes the total amount of revenue (`total_amount` field) for each taxi driver (`hack_license`). Then, sort your RDD by the total revenue in descending order and print out the top 5 lines with the largest total revenue. That shows the 5 taxi drivers with most total revenue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - compute total revenue by weekday through join operation (20 pts)\n",
    "\n",
    "Write a Spark program on your derived cleaned RDD (from task 1) that computes the total amount of revenue (`total_amount` field) for each 7 days of the week (Sunday through Saturday).\n",
    "\n",
    "To extract the week days and experimenting more with Spark, we suggest that you use `join` RDD operation to join the taxi dataset with the provided `2013_weekdays.csv` file that contains the dates for 365 days of the year 2013 and their corresponding week days.    \n",
    "\n",
    "First, read `2013_weekdays.csv` into an RDD, and split each line by `,` to extract the field values.\n",
    "\n",
    "Then, manipulate this RDD and your derived cleaned RDD of taxi dataset (from task 1), to be able to join the two and compute the total revenue by weekday.  \n",
    "\n",
    "Finally, sum the total amount per weekday, and return the result in descending order of the total revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - run on a big file in EMR cluster (20 pts)\n",
    "\n",
    "For the last part of this homework, you need to run your Spark code for tasks 1 through 4, on a big file in S3, in an AWS EMR cluster. \n",
    "\n",
    "Follow the instructions on `Lab - Spark Intro (AWS)` to create and connect to an EMR cluster in AWS and run Spark programs in there. \n",
    "\n",
    "**For better efficiency, in the hardware configuration of your cluster, choose `m5.xlarge` as instance type, and type 4 as the number of instances.**  \n",
    "\n",
    "The big file exists in this S3 URI: `s3://comp643bucket/homework/spark_taxicab/trip*.csv`\n",
    "\n",
    "To read the big file from S3 into an RDD, use the code below:\n",
    "\n",
    "`taxi = sc.textFile (\"s3://comp643bucket/homework/spark_taxicab/trip*.csv\")`\n",
    "\n",
    "Repeat tasks 1 through 4 on this `taxi` RDD created from the big file, and print your results in the markdown cells below (keep the results from the small subset above). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat task 1 on the big file in your EMR cluster - print the number of lines (`count()`) of your cleaned RDD from the big file, here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your result in this markdown cell ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat task 2 on the big file in your EMR cluster - copy your result, which is the 5 dates with least total revenue, from the big file, here:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your result in this markdown cell ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat task 3 on the big file in your EMR cluster - copy your result, which is the top 5 drivers with the most revenue, from the big file, here:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your result in this markdown cell ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat task 4 on the big file in your EMR cluster. `2013_weekdays.csv` is also available in S3 through this URI `s3://comp643bucket/homework/spark_taxicab/2013_weekdays.csv`. Copy your result, which is the sum of revenue per weekday in descending order of total revenue, from the big file, here:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your result in this markdown cell ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
